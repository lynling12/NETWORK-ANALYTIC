{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Analytics\n",
    "\n",
    "+ Capture Twitter Data\n",
    "+ Analyze Twitter Data by Applying Descriptive, Content, & Network Analytics Techniques\n",
    "+ Present & Communicate The Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images\\twitteranalytics.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Twitter Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Twitter App and Obtain OAuth Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, make sure you have a Twitter Application. https://dev.twitter.com/ In the network analytics module, we created a Twitter app to use NodeXL (you can watch my video on this)). You can reuse that app for this task. You need to have the following information:\n",
    "\n",
    "- consumer_key\n",
    "- consumer_secret \n",
    "- access_token \n",
    "- access_token_secret \n",
    "\n",
    "second, tweepy python package is needed for this tutorial.\n",
    "\n",
    "- **pip install tweepy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Twitter data and save it in a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- adapted from http://adilmoujahid.com/posts/2014/07/twitter-analytics/\n",
    "\n",
    "- Create a python file (you may use **NotePad+** or PyCharm for this) and name it as **twitter_streaming.py**\n",
    "- If you run the program from your terminal using the command: **python twitter_streaming.py**, you will see data flowing like the picture below and also find a json file named as **politics.json**.\n",
    "- You can stop the program by pressing **Ctrl-C** or simply **closing the command prompt window**.\n",
    "- We want to capture this data into a file that we will use later for the analysis. You can do so by piping the output to a file using the following command: python twitter_streaming.py > twitter_data.txt."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If you're collecting tweets with very popular keywords (e.g., clinton, trump), use the following script.\n",
    "\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API\n",
    "consumer_key = 'insert your own key here'\n",
    "consumer_secret = 'insert your own key here'\n",
    "access_token = 'insert your own key here'\n",
    "access_token_secret = 'insert your own key here'\n",
    "\n",
    "save_file = open('politics.json', 'a')\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            # An attempt to cut down on non English tweets.\n",
    "            if data['user']['lang'] != 'en':\n",
    "                return\n",
    "\n",
    "\n",
    "        print data\n",
    "        save_file.write(str(data))\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "while True:\n",
    "\ttry:\n",
    "\t\tstream = Stream(auth, l)\n",
    "\t\t#This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby', '#bigdata'\n",
    "\t\tstream.filter(track=['clinton', 'trump'])\n",
    "\texcept: \n",
    "\t\tcontinue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you receive an error \"data must be a byte string\"\n",
    "\n",
    "Install pyOpenSSL 0.15.1\n",
    "\n",
    "- Open a terminal in Mac or a command prompt in Windows\n",
    "- then, type **pip install pyopenssl**\n",
    "\n",
    "https://pypi.python.org/pypi/pyOpenSSL\n",
    "\n",
    "This will fix the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieve and Process Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/politics.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-139c21bbdaac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/politics.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# read only the first tweet/line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load it as Python dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/politics.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "with open('data/politics.json', 'r') as f:\n",
    "    line = f.readline() # read only the first tweet/line\n",
    "    tweet = json.loads(line) # load it as Python dictionary\n",
    "    print(json.dumps(tweet, indent=4)) \n",
    "    \n",
    "# the original data from Twitter looks like below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The key attributes are the following:\n",
    "\n",
    "text: the text of the tweet itself\n",
    "\n",
    "created_at: the date of creation\n",
    "\n",
    "favorite_count, retweet_count: the number of favourites and retweets\n",
    "\n",
    "favorited, retweeted: boolean stating whether the authenticated user (you) have favourited or retweeted this tweet\n",
    "\n",
    "lang: acronym for the language (e.g. “en” for english)\n",
    "\n",
    "id: the tweet identifier\n",
    "\n",
    "place, coordinates, geo: geo-location information if available\n",
    "\n",
    "user: the author’s full profile\n",
    "\n",
    "entities: list of entities like URLs, @-mentions, hashtags and symbols\n",
    "\n",
    "in_reply_to_user_id: user identifier if the tweet is a reply to a specific user\n",
    "\n",
    "in_reply_to_status_id: status identifier id the tweet is a reply to a specific status\n",
    "\n",
    "http://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# create an empty list to store our tweets in\n",
    "data = []\n",
    "\n",
    "# append each line of the data to our tweets list using the json module\n",
    "for line in open('data/politics.json'):\n",
    "    try:\n",
    "        data.append(json.loads(line))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# lets see how many we got\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the first five tweets and other meta data only\n",
    "for i in data[:5]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read first five tweets only \n",
    "for i in data[:5]:\n",
    "    print i['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# savin the entire tweets (not other data) in a variable and print it\n",
    "\n",
    "texts = [ T['text'] for T in data if 'text' in T ]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for T in data:\n",
    "    if 'text' not in T:\n",
    "        print T    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what happened. Twitter sets a limit on how many requests your Twitter app can make to Twitter. The above are the rate limiting messages from Twitter. It appears that we searched two terms \"hillary\" and \"trump\". There are so many tweets containing either name so we experiences the rate limit set by Twitter. In the future you would need a single keyword search if the term is so popular in Twitter.\n",
    "\n",
    "Then, the solution is removing those rate limiting messages from your original data.\n",
    "\n",
    "If your search term is not popular (e.g., supplychain, informationsystems, HR), you won't have this issue at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in data[:5]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing those 22 error messages\n",
    "tweets = []\n",
    "for T in data:\n",
    "    if 'text' in T:\n",
    "        tweets.append(T)\n",
    "len(tweets)       \n",
    "#now we have 8902 ... good!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save screen_names\n",
    "\n",
    "screen_names = [T['user']['screen_name'] for T in tweets]\n",
    "len(screen_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display screen_name, tweets\n",
    "\n",
    "for i in tweets[:5]:\n",
    "    print i['user']['screen_name'], i['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# More codes for extracting information from tweets\n",
    "\n",
    "ids = [T['id_str'] for T in tweets]\n",
    "times = [T['created_at'] for T in tweets]\n",
    "texts = [T['text'] for T in tweets]\n",
    "screen_names = [T['user']['screen_name'] for T in tweets]\n",
    "names = [T['user']['name'] for T in tweets]\n",
    "lats = [(T['geo']['coordinates'][0] if T['geo'] else None) for T in tweets]\n",
    "lons = [(T['geo']['coordinates'][1] if T['geo'] else None) for T in tweets]\n",
    "place_names = [(T['place']['full_name'] if T['place'] else None) for T in tweets]\n",
    "place_types = [(T['place']['place_type'] if T['place'] else None) for T in tweets]\n",
    "\n",
    "# open an output csv file to write to\n",
    "out = open('tweets_food.csv', 'w')\n",
    "\n",
    "# write the header of our CSV as its first line\n",
    "out.write('id,created at,text,screen name,name,lat,lon,place name,place type\\n')\n",
    "\n",
    "# merge each individual list into a single list using the zip function\n",
    "rows = zip(ids, times, texts, screen_names, names, lats, lons, place_names, place_types)\n",
    "\n",
    "# use the writer module on our csv file\n",
    "csv = writer(out)\n",
    "\n",
    "# use one value from each of our rows list and write it to the csv as a new row\n",
    "for row in rows:\n",
    "    values = [(value.encode('utf8') if hasattr(value, 'encode') else value) for value in row]\n",
    "    csv.writerow(values)\n",
    "\n",
    "# close our csv file when done\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Descriptive Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import popular packages\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(screen_names)\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many unique users in the data?\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many tweets per user?\n",
    "\n",
    "float(2657/2303)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(screen_names)\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# five most active tweeters\n",
    "c.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make it pretty\n",
    "activetweeters = c.most_common(5)\n",
    "activetweeters_df = pd.DataFrame(activetweeters)\n",
    "activetweeters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = [T['user']['lang'] for T in tweets if 'user' in T]\n",
    "\n",
    "c = Counter(lang)\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only English tweets & meta data\n",
    "english = []\n",
    "for i in tweets:\n",
    "    if i['user']['lang'] == \"en\":\n",
    "        english.append(i)\n",
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read first five English tweets only \n",
    "for i in english[:5]:\n",
    "    print i['text']\n",
    "    \n",
    "# so now all English tweets are saved in english. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who is sharing location information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many tweets contain geocode\n",
    "\n",
    "geo = [T['user']['geo_enabled'] for T in tweets if 'user' in T]\n",
    "\n",
    "c = Counter(geo)\n",
    "print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original tweets & Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove retweets\n",
    "\n",
    "originaltweets = []\n",
    "\n",
    "for tweet in texts:\n",
    "    if 'rt @' not in tweet.lower():\n",
    "        originaltweets.append(tweet)\n",
    "        \n",
    "len(originaltweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get retweets only\n",
    "\n",
    "#remove retweets\n",
    "\n",
    "retweets_only = []\n",
    "\n",
    "for tweet in texts:\n",
    "    if 'rt @' in tweet.lower():\n",
    "        retweets_only.append(tweet)\n",
    "        \n",
    "len(retweets_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in retweets_only[:5]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most visible users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in texts[:5]:\n",
    "    print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first extract all users from tweets\n",
    "\n",
    "#let's use regular expression ... how to use re ... https://docs.python.org/2/howto/regex.html\n",
    "    \n",
    "import re\n",
    "\n",
    "for tweet in texts[:5]:\n",
    "    print re.findall(r\"(?<=@)\\w+\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in texts[:5]:\n",
    "    a = re.findall(r\"(?<=@)\\w+\", tweet)\n",
    "    for i in a:\n",
    "        print '@'+i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visible_users = []\n",
    "\n",
    "for tweet in texts:\n",
    "    a = re.findall(r\"(?<=@)\\w+\", tweet)\n",
    "    for i in a:\n",
    "        visible_users.append(['@'+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute frequency distribution for visible users in the tweets\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#convert lists to strings\n",
    "visible_users = str(visible_users)\n",
    "\n",
    "#lowercase\n",
    "visible_users = visible_users.lower()\n",
    "\n",
    "#tokenize\n",
    "visible_users = visible_users.split()\n",
    "\n",
    "fdist = nltk.FreqDist(visible_users)\n",
    "\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for T in tweets[:10]:\n",
    "    print T['entities']['urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for T in tweets:\n",
    "    for i in T['entities']['urls']:\n",
    "        print i['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "for T in tweets:\n",
    "    for i in T['entities']['urls']:\n",
    "        urls.append(i['url'])\n",
    "        \n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#top 10 urls ... visit some of them and find out what the articles are about\n",
    "\n",
    "c = Counter(urls)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Data Preprocessing\n",
    "- remove urls\n",
    "- remove user names\n",
    "- extract only urls (for url frequency analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove urls\n",
    "\n",
    "texts_wo_urls = []\n",
    "\n",
    "for i in texts:\n",
    "    result = re.sub(r\"http\\S+\", \"\", i)\n",
    "    texts_wo_urls.append(result)\n",
    "\n",
    "texts_wo_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove user names\n",
    "\n",
    "texts_wo_urls_usernames = []\n",
    "\n",
    "for i in texts_wo_urls:\n",
    "    result = re.sub(r\"(@[A-Za-z0-9]+)\", \"\", i)\n",
    "    texts_wo_urls_usernames.append(result)\n",
    "\n",
    "texts_wo_urls_usernames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_clean_completely = []\n",
    "\n",
    "for i in texts:\n",
    "    result = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", i)\n",
    "    texts_clean_completely.append(result)\n",
    "\n",
    "texts_clean_completely[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this would be better ...\n",
    "texts_clean_completely2 = []\n",
    "\n",
    "for i in texts:\n",
    "    result = ' '.join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", i).split())\n",
    "    texts_clean_completely2.append(result)\n",
    "\n",
    "texts_clean_completely2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data would be good for content analytics (e.g., word frequency, clustering analysis). For content analytics, you need to go through text preprocessing steps (e.g., tokenization, remove stopwords, remove short words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract urls from texts (rather than the original JSON data)\n",
    "\n",
    "only_urls = []\n",
    "\n",
    "for i in texts:\n",
    "    result = re.findall(r'(https?://\\S+)', i)\n",
    "    for url in result: \n",
    "        only_urls.append(url)\n",
    "\n",
    "only_urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = Counter(only_urls)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the total number of urls in the tweets\n",
    "len(only_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the number of unique urls in the tweets\n",
    "len(c.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for more descriptive analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "screen_names = [T['user']['screen_name'] for T in tweets if 'user' in T]\n",
    "screen_names_description = [status['user']['description'] for status in tweets if 'user' in status]\n",
    "followers_count = [status['user']['followers_count'] for status in tweets if 'user' in status]\n",
    "friends_count = [status['user']['friends_count'] for status in tweets if 'user' in status]\n",
    "screen_names_created = [status['user']['created_at'] for status in tweets if 'user' in status]\n",
    "location = [status['user']['location'] for status in tweets if 'user' in status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "followers_friends = zip(screen_names, followers_count, friends_count)\n",
    "for i in followers_friends:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is another way to find out screen name, follower count, and friends count\n",
    "\n",
    "for tweet in tweets:\n",
    "    print tweet['user']['screen_name'], tweet['user']['followers_count'], tweet['user']['friends_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving every user and his/her follower counts\n",
    "\n",
    "user_followerscount = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    user_followerscount.append([tweet['user']['screen_name'], tweet['user']['followers_count']]) \n",
    "    \n",
    "user_followerscount[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted(user_followerscount,key=itemgetter(1), reverse=True)\n",
    "#sorted(user_followerscount,key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do people live?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in location:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Content Analytics (Text Mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#convert lists to strings\n",
    "originaltweets = str(originaltweets)\n",
    "\n",
    "#lowercase\n",
    "tokens = originaltweets.lower()\n",
    "\n",
    "#tokenize\n",
    "tokens = tokens.split()\n",
    "\n",
    "#Remove stopwords\n",
    "tokens = (word for word in tokens if word not in stopwords.words('english'))\n",
    "\n",
    "# Filter non-alphanumeric chars from tokens\n",
    "tokens = (ch.lower() for ch in tokens if ch.isalpha())\n",
    "\n",
    "#Create your bigrams\n",
    "#bgs = nltk.bigrams(tokens)\n",
    "\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "#k refers to keys (or tokens); v refers to values (or counts)\n",
    "for k,v in fdist.items()[:10]:\n",
    "    print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hashtag Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in texts[:5]:\n",
    "    print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first extract all hashtags from tweets\n",
    "\n",
    "import re\n",
    "\n",
    "hashtags = []\n",
    "\n",
    "for tweet in texts[:50]:\n",
    "    print re.findall(r\"(?<=#)\\w+\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list one hashtag in a row and save them\n",
    "for tweet in texts[:50]:\n",
    "    a = re.findall(r\"(?<=#)\\w+\", tweet)\n",
    "    for i in a:\n",
    "        hashtags.append(['#'+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in hashtags[:15]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute frequency distribution for all the hashtags in the tweets\n",
    "\n",
    "#convert lists to strings\n",
    "hashtags_string = str(hashtags)\n",
    "\n",
    "#lowercase\n",
    "hashtags_string = hashtags_string.lower()\n",
    "\n",
    "#tokenize\n",
    "hashtags_string = hashtags_string.split()\n",
    "\n",
    "fdist = nltk.FreqDist(hashtags_string)\n",
    "\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Topic Modeling\n",
    "\n",
    "+ Below is a simple demnostration of topic modeling on a sample data from Twitter. For some serious analysis for a business, you need to have a **comprehensive** dataset and the dataset needs to be properly processed through removing stopwords, stemming (or lemmatizing), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you should be able to do document clustering and/or topic modeling here\n",
    "# select the English tweets only for topic modeling or document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# import packages for text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.wrappers import LdaVowpalWabbit, LdaMallet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read first five English tweets only \n",
    "for i in english[:5]:\n",
    "    print i['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select original English tweets (exclusing retweets in clustering analysis)\n",
    "\n",
    "english_originaltweets = []\n",
    "\n",
    "for tweet in english:\n",
    "    if 'rt @' not in tweet['text'].lower():\n",
    "        english_originaltweets.append(tweet['text'])\n",
    "        \n",
    "len(english_originaltweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_originaltweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove useless numbers and alphanumerical words including http     \n",
    "documents = [re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text) for text in english_originaltweets]\n",
    "# tokenize\n",
    "texts = [[word for word in text.lower().split() ] for text in documents]\n",
    "# stemming words: having --> have; friends --> friend\n",
    "lmtzr = WordNetLemmatizer()\n",
    "texts = [[lmtzr.lemmatize(word) for word in text ] for text in texts]\n",
    "# remove common words \n",
    "stoplist = stopwords.words('english')\n",
    "texts = [[word for word in text if word not in stoplist] for text in texts]\n",
    "#remove short words\n",
    "english_originaltweets_clean = [[ word for word in tokens if len(word) >= 3 ] for tokens in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A list of extra stopwords specific to the debates transcripts (if you want to remove more stopwords)\n",
    "extra_stopwords = ['amp','get','got','hey','hmm','hoo','hop','iep','let','ooo','par',\n",
    "            'pdt','pln','pst','wha','yep','yer','aest','didn','nzdt','via',\n",
    "            'one','com','new','like','great','make','top','awesome','best',\n",
    "            'good','wow','yes','say','yay','would','thanks','thank','going',\n",
    "            'new','use','should','could','really','see','want','nice',\n",
    "            'while','know','free','today','day','always','last','put','live',\n",
    "            'week','went','wasn','was','used','ugh','try','kind', 'http','much',\n",
    "            'need', 'next','app','ibm','appleevent','using']\n",
    "\n",
    "extra_stoplist = extra_stopwords\n",
    "english_originaltweets_clean = [[word for word in text if word not in extra_stoplist] for text in english_originaltweets_clean]\n",
    "#https://github.com/alexperrier/datatalks/blob/master/debates/R/stm.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_originaltweets_clean[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# after processing each tweet, some tweets could be empty. These empty rows should be removed from further analysis.\n",
    "\n",
    "english_originaltweets_clean = [x for x in english_originaltweets_clean if x]\n",
    "english_originaltweets_clean[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(english_originaltweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is text processing required for topic modeling with Gensim\n",
    "dictionary = Dictionary(english_originaltweets_clean)\n",
    "corpus = [dictionary.doc2bow(text) for text in english_originaltweets_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(1) # setting random seed to get the same results each time.\n",
    "k_range = range(2,20)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    goodLdaModel = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, passes=75)\n",
    "    goodcm = CoherenceModel(model=goodLdaModel, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "    scores.append(goodcm.get_coherence())\n",
    "    \n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(k_range, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(1) # setting random seed to get the same results each time. For a large dataset, high passes (75) would be desirable.\n",
    "model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=4, passes=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be a lot of extra stopwords (e.g., trump, donald, said, come), which should be removed prior to topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print words without probability\n",
    "for i in range(0,4):\n",
    "    topics = model.show_topic(i, 10)\n",
    "    print ', '.join([str(word[0]) for word in topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_corpus = model[corpus]\n",
    "\n",
    "results = []\n",
    "for i in lda_corpus:\n",
    "    #print i\n",
    "    results.append(i)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for i in english_originaltweets_clean:\n",
    "    documents.append(str(i).replace(\",\", \"\").replace(\"u'\",\"\").replace(\"'\", \"\"))\n",
    "\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finding highest value from each row\n",
    "toptopic = [max(collection, key=lambda x: x[1])[0] for collection in results]\n",
    "\n",
    "toptopic = pd.DataFrame(toptopic)\n",
    "documents = pd.DataFrame(documents)\n",
    "documents = documents.rename(columns = {0: 'documents'})\n",
    "summary = documents.join(toptopic)\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary.groupby(0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you should be able to do unsupervised sentiment analysis here ...\n",
    "# select the English tweets only for sentiment analysis\n",
    "# you may use \"Pattern python package\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in english_originaltweets_clean:\n",
    "    score = sentiment(tweet)\n",
    "    print score[0], score[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could do text preprocessing before sentiment analysis. The results look almost same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove useless numbers and alphanumerical words\n",
    "documents = [re.sub(\"[^a-zA-Z]+\", \" \", document) for document in english_originaltweets]\n",
    "# tokenize\n",
    "texts = [[word for word in document.lower().split() ] for document in documents]\n",
    "# remove common words \n",
    "stoplist = stopwords.words('english')\n",
    "texts = [[word for word in text if word not in stoplist] for text in texts]\n",
    "#remove short words\n",
    "texts = [[ word for word in tokens if len(word) >= 3 ] for tokens in texts]\n",
    "\n",
    "for row in texts:\n",
    "    score = sentiment(row)\n",
    "    print score[0], score[1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting and analysting user profiles (Appedix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in screen_names_description:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Network Analytics \n",
    "\n",
    "## Mention Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a tweet by **@kevin**, **\"@amy, are you available today? how about coffee this afternoon? #friday**\n",
    "\n",
    "The above tweet creates a relationship between @kevin and @amy. This relationship is created by **mention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in tweets[:5]:\n",
    "    print tweet['user']['screen_name']+',', tweet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mention = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    mention.append([tweet['user']['screen_name']+',', tweet['text']])\n",
    "    \n",
    "mention[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computationally intensive ... very slow if you have a lot of data\n",
    "for i in mention:\n",
    "    print i[0], i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in mention:\n",
    "    print tweet[0], re.findall(r\"(?<=@)\\w+\", tweet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in mention:\n",
    "    a = re.findall(r\"(?<=@)\\w+\", tweet[1])\n",
    "    for i in a:\n",
    "        print tweet[0], '@'+i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#putting everything together\n",
    "\n",
    "import csv\n",
    "\n",
    "mention = []\n",
    "for tweet in tweets:\n",
    "    mention.append([tweet['user']['screen_name']+',', tweet['text']])    \n",
    "openfile = open(\"data/mentionnetwork.csv\", \"wb\")\n",
    "\n",
    "w = csv.writer(openfile)\n",
    "for tweet in mention:\n",
    "    a = re.findall(r\"(?<=@)\\w+\", tweet[1])\n",
    "    for i in a:\n",
    "        w.writerow([tweet[0], '@'+i])\n",
    "        \n",
    "openfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mentionnetwork.csv contains two columns representing relationships. Now. you can use **NodeXL** or **Gephi** for **network analytics**.\n",
    "\n",
    "You should perform various statistical analyses (e.g., centrality, degree) and modularity analysis using Gephi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling using Co-Hashtag Network Analysis\n",
    "\n",
    "- Identify co-appearing hashtags and build a network of co-appearing hashtags\n",
    "- Apply modularity (or network clustering) analysis and identify topics or themes\n",
    "- This is similar to topic modeling: the difference is that topic modeling uses texts, and this proposed approach uses hashtags and network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in tweets[:2]:\n",
    "    print tweet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_originaltweets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in english_originaltweets[:2]:\n",
    "    print re.sub(\"[^a-zA-Z0-9#]+\", \" \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in english_originaltweets[:10]:\n",
    "    data = re.sub(\"[^a-zA-Z0-9#]+\", \" \", i)\n",
    "    hashtag = re.findall(r\"(?<=#)\\w+\", str(data).lower())\n",
    "    print hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas = []\n",
    "for i in english_originaltweets:\n",
    "    data = re.sub(\"[^a-zA-Z0-9#]+\", \" \", i)\n",
    "    hashtag = re.findall(r\"(?<=#)\\w+\", str(data).lower())\n",
    "    datas.append(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "cohashtags = [x for d in datas for x in combinations(d, 2)]\n",
    "cohashtags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for cohashtag analysis\n",
    "outfile = open(\"data/cohashtag_network.csv\", \"wb\")\n",
    "w = csv.writer(outfile)\n",
    "for i in cohashtags:\n",
    "    w.writerow(i)    \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Spatial Analytics (Appendix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lats = [(T['geo']['coordinates'][0] if T['geo'] else None) for T in tweets]\n",
    "lons = [(T['geo']['coordinates'][1] if T['geo'] else None) for T in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geo = zip(lats, lons)\n",
    "geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "openfile = open(\"data/geo.csv\", \"wb\")\n",
    "w = csv.writer(openfile)\n",
    "for i in geo:\n",
    "    w.writerow([i])      \n",
    "openfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open Excel and import the csv data (using **import texts** in Data Tab). Make sure that the result is an Excel file with two columns (latitute, longitude)\n",
    "\n",
    "Remove the rows with NONE. \n",
    "\n",
    "Import the Excel file in **Tableau** and visualize the location data in a map. That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
